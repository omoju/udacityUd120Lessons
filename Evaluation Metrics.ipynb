{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 14 - Evaluation Metrics\n",
    "\n",
    "**Task:** Identify *Persons Of Interest* (POI) for Enron fraud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 95\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "data_dict = pickle.load(open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### first element is our labels, any added elements are predictor\n",
    "### features. Keep this the same for the mini-project, but you'll\n",
    "### have a different feature list when you do the final project.\n",
    "features_list = [\"poi\", \"salary\"]\n",
    "\n",
    "data = featureFormat(data_dict, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "print len(labels), len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a decision tree classifier (just use the default parameters), train it on all the data. Print out the accuracy. \n",
    "THIS IS AN OVERFIT TREE, DO NOT TRUST THIS NUMBER! Nonetheless, \n",
    "- what’s the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.001s\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from time import time\n",
    "\n",
    "def submitAcc(features, labels):\n",
    "    return clf.score(features, labels)\n",
    "\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "t0 = time()\n",
    "clf.fit(features, labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with accurancy 0.99%\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(features)\n",
    "print \"Classifier with accurancy %.2f%%\" % (submitAcc(features, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now you’ll add in training and testing, so that you get a trustworthy accuracy number. Use the train_test_split validation available in sklearn.cross_validation; hold out 30% of the data for testing and set the random_state parameter to 42 (random_state controls which points go into the training set and which are used for testing; setting it to 42 means we know exactly which events are in which set, and can check the results you get). \n",
    "- What’s your updated accuracy?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 66\n",
      "29 29\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, labels, test_size=0.30, random_state=42)\n",
    "\n",
    "print len(X_train), len(y_train)\n",
    "print len(X_test), len(y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.001s\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "t0 = time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with accurancy 0.72%\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "print \"Classifier with accurancy %.2f%%\" % (submitAcc(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many POIs are in the test set for your POI identifier?\n",
    "- (Note that we said test set! We are not looking for the number of POIs in the whole dataset.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "numPoiInTestSet = len([p for p in y_test if p == 1.0])\n",
    "print numPoiInTestSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your identifier predicted 0. (not POI) for everyone in the test set, what would its accuracy be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8620689655172413"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "1.0 - numPoiInTestSet/29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaaand the testing data brings us back down to earth after that 99% accuracy.\n",
    "\n",
    "## Concerns with *Accuracy*\n",
    "- If you have a **skewed** dataset, as is the case with this dataset\n",
    "- The problem might be of such that it is best to err on the side of guessing innocence\n",
    "- For another case, you may want to err on the side of predicting guilt, with the hopes that the innocent persons will be cleared through the investigation.\n",
    "\n",
    "Accuracy is not particularly good if any of these cases apply to you.  **Precision** and **recall** are a better metric for evaluating the performance of the model.\n",
    "\n",
    "## Picking The Most Suitable Metric\n",
    "\n",
    "As you may now see, having imbalanced classes like we have in the Enron dataset (many more non-POIs than POIs) introduces some special challenges, namely that you can just guess the more common class label for every point, not a very insightful strategy, and still get pretty good accuracy!\n",
    "\n",
    "Precision and recall can help illuminate your performance better. \n",
    "- Use the `precision_score` and `recall_score` available in `sklearn.metrics` to compute those quantities.\n",
    "- What’s the precision?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test,clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously this isn’t a very optimized machine learning strategy (we haven’t tried any algorithms besides the decision tree, or tuned any parameters, or done any feature selection), and now seeing the precision and recall should make that much more apparent than the accuracy did.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test,clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                            Actual Class\n",
      "           Predicted                                Positive               Negative\n",
      "                                Positive              21.000                  4.000\n",
      "                                Negative               4.000                  0.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_true = y_test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cM = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print \"{:>72}\".format('Actual Class')\n",
    "print \"{:>20}{:>20}{:>20}{:>23}\".format('Predicted', '', 'Positive', 'Negative')\n",
    "print \"{:>20}{:>20}{:>20.3f}{:>23.3f}\".format('', 'Positive', cM[0][0], cM[0][1])\n",
    "print \"{:>20}{:>20}{:>20.3f}{:>23.3f}\".format('', 'Negative', cM[1][0], cM[1][1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
